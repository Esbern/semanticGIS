{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Downloading SDL for DAR...\n",
      "[+] Saved 36 entities to schemas/dar.json\n",
      "[*] Downloading SDL for BBR...\n",
      "[+] Saved 48 entities to schemas/bbr.json\n",
      "[*] Downloading SDL for DHMOprindelse...\n",
      "[+] Saved 17 entities to schemas/dhmoprindelse.json\n",
      "[*] Downloading SDL for DHMHoejdekurver...\n",
      "[+] Saved 18 entities to schemas/dhmhoejdekurver.json\n",
      "[*] Downloading SDL for MAT...\n",
      "[+] Saved 117 entities to schemas/mat.json\n",
      "[*] Downloading SDL for ESB...\n",
      "[!] Failed to prepare schema for ESB: Schema download for ESB failed (404): \n",
      "[*] Downloading SDL for FIKSPUNKT...\n",
      "[+] Saved 124 entities to schemas/fikspunkt.json\n",
      "[*] Downloading SDL for DS...\n",
      "[+] Saved 107 entities to schemas/ds.json\n",
      "[*] Downloading SDL for GEODKV...\n",
      "[+] Saved 218 entities to schemas/geodkv.json\n",
      "[*] Downloading SDL for CVR...\n",
      "[+] Saved 56 entities to schemas/cvr.json\n",
      "[*] Downloading SDL for CVR/custom...\n",
      "[+] Saved 4 entities to schemas/cvr_custom.json\n",
      "[*] Downloading SDL for EJF...\n",
      "[+] Saved 32 entities to schemas/ejf.json\n",
      "[*] Downloading SDL for GEODKV...\n",
      "[+] Saved 218 entities to schemas/geodkv.json\n",
      "[*] Downloading SDL for CPR...\n",
      "[+] Saved 83 entities to schemas/cpr.json\n",
      "[*] Downloading SDL for SVR...\n",
      "[+] Saved 41 entities to schemas/svr.json\n",
      "[*] Downloading SDL for VUR...\n",
      "[+] Saved 41 entities to schemas/vur.json\n",
      "[*] Downloading SDL for HISTKORT...\n",
      "[+] Saved 5 entities to schemas/histkort.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from graphql import GraphQLList, GraphQLNonNull, GraphQLObjectType, build_schema\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\n",
    "        \"Missing dependency 'graphql-core'. Install it via `pip install graphql-core`.\"\n",
    "    ) from exc\n",
    "\n",
    "\n",
    "class SemanticScraper:\n",
    "    SCHEMA_URL_TEMPLATE = \"https://graphql.datafordeler.dk/{register}/v1/schema\"\n",
    "\n",
    "    def __init__(self, api_key, output_dir=\".\"):\n",
    "        self.api_key = api_key\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # Mapping GraphQL scalar names to semanticGIS pipeline primitives\n",
    "        self.type_map = {\n",
    "            \"CharacterString\": \"str\",\n",
    "            \"String\": \"str\",\n",
    "            \"UUID\": \"str\",\n",
    "            \"ID\": \"str\",\n",
    "            \"Int\": \"int\",\n",
    "            \"Long\": \"int\",\n",
    "            \"Float\": \"float\",\n",
    "            \"Decimal\": \"float\",\n",
    "            \"Boolean\": \"bool\",\n",
    "            \"DafDateTime\": \"datetime\",\n",
    "            \"DateTime\": \"datetime\",\n",
    "            \"Date\": \"date\",\n",
    "        }\n",
    "\n",
    "    def fetch_schema_sdl(self, register):\n",
    "        \"\"\"Downloads the GraphQL SDL for a register.\"\"\"\n",
    "        url = self.SCHEMA_URL_TEMPLATE.format(register=register)\n",
    "        print(f\"[*] Downloading SDL for {register}...\")\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            params={\"apiKey\": self.api_key},\n",
    "            timeout=60,\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"Schema download for {register} failed ({response.status_code}): {response.text}\"\n",
    "            )\n",
    "        return response.text\n",
    "\n",
    "    def resolve_base_type(self, gql_type):\n",
    "        \"\"\"Removes LIST/NON_NULL wrappers and returns the underlying type name.\"\"\"\n",
    "        while isinstance(gql_type, (GraphQLList, GraphQLNonNull)):\n",
    "            gql_type = gql_type.of_type\n",
    "        return getattr(gql_type, \"name\", str(gql_type))\n",
    "\n",
    "    def build_registry(self, register, schema):\n",
    "        registry = {\n",
    "            \"register\": register,\n",
    "            \"entities\": {},\n",
    "        }\n",
    "        skip_types = {\"Query\", \"Mutation\", \"Subscription\"}\n",
    "\n",
    "        for type_name, gql_type in schema.type_map.items():\n",
    "            if type_name.startswith(\"__\") or type_name in skip_types:\n",
    "                continue\n",
    "            if not isinstance(gql_type, GraphQLObjectType):\n",
    "                continue\n",
    "\n",
    "            entity_info = {\n",
    "                \"da_description\": gql_type.description or \"\",\n",
    "                \"attributes\": {},\n",
    "            }\n",
    "\n",
    "            for field_name, field in gql_type.fields.items():\n",
    "                base_type = self.resolve_base_type(field.type)\n",
    "                clean_type = self.type_map.get(base_type, \"str\")\n",
    "                entity_info[\"attributes\"][field_name] = {\n",
    "                    \"da_description\": field.description or \"\",\n",
    "                    \"tech_type\": clean_type,\n",
    "                    \"graphql_type\": base_type,\n",
    "                }\n",
    "\n",
    "            registry[\"entities\"][type_name] = entity_info\n",
    "\n",
    "        return registry\n",
    "\n",
    "    def process_register(self, register):\n",
    "        try:\n",
    "            schema_sdl = self.fetch_schema_sdl(register)\n",
    "            schema = build_schema(schema_sdl)\n",
    "        except Exception as exc:\n",
    "            print(f\"[!] Failed to prepare schema for {register}: {exc}\")\n",
    "            return\n",
    "\n",
    "        registry = self.build_registry(register, schema)\n",
    "        output_file = self.output_dir / f\"{register.lower().replace('/', '_')}.json\"\n",
    "        with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(registry, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"[+] Saved {len(registry['entities'])} entities to {output_file}\")\n",
    "\n",
    "\n",
    "# --- EXECUTION ---\n",
    "REGISTERS = [\"DAR\", \"BBR\", \"DHMOprindelse\", \"DHMHoejdekurver\",\"MAT\",\"ESB\",\"FIKSPUNKT\", \"DS\",\"GEODKV\",\"CVR\",\"CVR/custom\", \"EJF\", \"GEODKV\",\"CPR\",\"SVR\",\"VUR\",\"HISTKORT\"]\n",
    "MY_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "OUTPUT_DIR = \"schemas\"\n",
    "\n",
    "scraper = SemanticScraper(MY_API_KEY, output_dir=OUTPUT_DIR)\n",
    "\n",
    "for reg in REGISTERS:\n",
    "    scraper.process_register(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d917c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "STANDARD_SCALARS = {\n",
    "    \"CharacterString\",\n",
    "    \"String\",\n",
    "    \"UUID\",\n",
    "    \"ID\",\n",
    "    \"Int\",\n",
    "    \"Integer\",\n",
    "    \"Long\",\n",
    "    \"Float\",\n",
    "    \"Decimal\",\n",
    "    \"Boolean\",\n",
    "    \"DafDateTime\",\n",
    "    \"DateTime\",\n",
    "    \"Date\",\n",
    "}\n",
    "\n",
    "EXCLUDED_ENTITY_TOKENS = [\n",
    "    \"Connection\",\n",
    "    \"Edge\",\n",
    "    \"Payload\",\n",
    "    \"Spatial\",\n",
    "    \"Geometry\",\n",
    "    \"PageInfo\",\n",
    "    \"Events\",\n",
    "]\n",
    "\n",
    "OUTPUT_BASE = Path(\"output/dk\")\n",
    "\n",
    "\n",
    "def slugify(value, allow_non_ascii=False):\n",
    "    \"\"\"Returns a filesystem-safe slug. Optionally preserves Danish letters.\"\"\"\n",
    "    if value is None:\n",
    "        value = \"\"\n",
    "    if allow_non_ascii:\n",
    "        norm = unicodedata.normalize(\"NFKC\", value)\n",
    "        pattern = r\"[^0-9a-zæøå]+\"\n",
    "    else:\n",
    "        norm = unicodedata.normalize(\"NFKD\", value)\n",
    "        norm = norm.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "        pattern = r\"[^0-9a-z]+\"\n",
    "    norm = norm.lower()\n",
    "    slug = re.sub(pattern, \"_\", norm).strip(\"_\")\n",
    "    return slug or norm\n",
    "\n",
    "\n",
    "def canonical_entity_name(entity_name, register_name):\n",
    "    \"\"\"Removes register prefixes (DAR_, etc.) and slugifies the result.\"\"\"\n",
    "    lowered = entity_name.casefold()\n",
    "    prefix = f\"{register_name.casefold()}_\"\n",
    "    if lowered.startswith(prefix):\n",
    "        lowered = lowered[len(prefix):]\n",
    "    return slugify(lowered)\n",
    "\n",
    "\n",
    "def display_entity_title(entity_name, register_name):\n",
    "    \"\"\"Removes register prefixes while preserving casing for presentation.\"\"\"\n",
    "    prefix = f\"{register_name}_\"\n",
    "    if entity_name.lower().startswith(prefix.lower()):\n",
    "        cleaned = entity_name[len(prefix):]\n",
    "    else:\n",
    "        cleaned = entity_name\n",
    "    return cleaned.replace('_', ' ').strip() or entity_name\n",
    "\n",
    "\n",
    "def clean_info(info_str):\n",
    "    \"\"\"Udtrækker de vigtige felter fra Grunddatamodel info-strengen.\"\"\"\n",
    "    if not info_str or \"Grunddatamodel info:\" not in info_str:\n",
    "        return {\"definition\": info_str.replace('\\n', ' ').strip() if info_str else \"\"}\n",
    "    \n",
    "    data = {}\n",
    "    lines = info_str.split('\\n')\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            key, val = line.split(':', 1)\n",
    "            clean_val = val.replace('<memo>#NOTES#', '').replace('#NOTES#', '').strip()\n",
    "            data[key.strip()] = clean_val\n",
    "            \n",
    "    return {\n",
    "        \"definition\": data.get(\"definition (da)\", \"Ingen definition fundet\").strip(),\n",
    "        \"type_clue\": data.get(\"type\", None),\n",
    "        \"legal\": data.get(\"legalSource\", None)\n",
    "    }\n",
    "\n",
    "\n",
    "def should_skip_entity(entity_name):\n",
    "    return any(token in entity_name for token in EXCLUDED_ENTITY_TOKENS)\n",
    "\n",
    "\n",
    "def register_output_dir(register_name):\n",
    "    parts = [part for part in register_name.split('/') if part]\n",
    "    if not parts:\n",
    "        parts = [register_name or \"register\"]\n",
    "    reg_dir = OUTPUT_BASE\n",
    "    for part in parts:\n",
    "        reg_dir /= part\n",
    "    return reg_dir\n",
    "\n",
    "\n",
    "def clean_register_dir(reg_dir):\n",
    "    if not reg_dir.exists():\n",
    "        return\n",
    "    for child in reg_dir.iterdir():\n",
    "        if child.is_file() and child.suffix == \".md\":\n",
    "            child.unlink()\n",
    "\n",
    "\n",
    "def generate_entity_markdown(register_name, entity_name, entity_data, entity_slug_set):\n",
    "    if should_skip_entity(entity_name):\n",
    "        return None\n",
    "\n",
    "    entity_info = clean_info(entity_data.get('da_description', ''))\n",
    "    reg_low = register_name.lower()\n",
    "    entity_slug = canonical_entity_name(entity_name, register_name)\n",
    "    entity_title = display_entity_title(entity_name, register_name)\n",
    "    md = f\"---\\ntitle: {entity_title}\\ndraft: false\\ntype: entity\\n---\\n\\n\"\n",
    "    md += f\"# {entity_title}\\n\\n{entity_info.get('definition', '')}\\n\\n\"\n",
    "\n",
    "    md += \"### Semantic Template\\n\"\n",
    "    md += \"```python\\n\"\n",
    "    md += \"p.io.declare_input(\\n\"\n",
    "    md += f\"    output_name=\\\"{entity_slug}\\\",\\n\"\n",
    "    md += \"    attributes={\\n\"\n",
    "    \n",
    "    standard_slugs = {slugify(s, allow_non_ascii=True) for s in STANDARD_SCALARS}\n",
    "\n",
    "    for attr_name, attr_data in entity_data.get('attributes', {}).items():\n",
    "        attr_info = clean_info(attr_data.get('da_description', ''))\n",
    "        type_clue = attr_info.get('type_clue')\n",
    "        type_slug = slugify(type_clue, allow_non_ascii=True) if type_clue else None\n",
    "        t_type = attr_data.get('tech_type', 'str')\n",
    "\n",
    "        if attr_name == \"id_lokalId\":\n",
    "            role = f\"sg.PK({t_type})\"\n",
    "            scale = \"MeasurementScale.NOMINAL\"\n",
    "        elif type_slug and type_slug in entity_slug_set:\n",
    "            role = f\"sg.FK(dk.{reg_low}.{type_slug}, {t_type})\"\n",
    "            scale = \"MeasurementScale.NOMINAL\"\n",
    "        elif type_slug and type_slug not in standard_slugs:\n",
    "            role = f\"sg.LOOKUP(dk.{reg_low}.lookups.{type_slug}, {t_type})\"\n",
    "            scale = \"MeasurementScale.NOMINAL\"\n",
    "        else:\n",
    "            role = f\"sg.DES({t_type})\"\n",
    "            if t_type in ['int', 'float']:\n",
    "                scale = \"MeasurementScale.RATIO\"\n",
    "            elif t_type == 'datetime':\n",
    "                scale = \"MeasurementScale.INTERVAL\"\n",
    "            else:\n",
    "                scale = \"MeasurementScale.NOMINAL\"\n",
    "\n",
    "        desc = attr_info.get('definition', '').replace('\"', \"'\").replace('\\n', ' ')\n",
    "        attr_slug = slugify(attr_name)\n",
    "        md += f\"        \\\"{attr_slug}\\\": {{\\n\"\n",
    "        md += f\"            \\\"scale\\\": {scale},\\n\"\n",
    "        md += f\"            \\\"role\\\": {role},\\n\"\n",
    "        md += f\"            \\\"description\\\": \\\"{desc}\\\"\\n\"\n",
    "        md += \"        },\\n\"\n",
    "    \n",
    "    md += \"    }\\n)\\n```\\n\"\n",
    "    return md\n",
    "\n",
    "\n",
    "def build_catalog(schema_folder):\n",
    "    schema_path = Path(schema_folder)\n",
    "    for json_path in sorted(schema_path.glob(\"*.json\")):\n",
    "        with json_path.open('r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        reg_name = data['register']\n",
    "        entity_slug_set = {\n",
    "            canonical_entity_name(entity_name, reg_name)\n",
    "            for entity_name in data['entities']\n",
    "            if not should_skip_entity(entity_name)\n",
    "        }\n",
    "\n",
    "        reg_dir = register_output_dir(reg_name)\n",
    "        reg_dir.mkdir(parents=True, exist_ok=True)\n",
    "        clean_register_dir(reg_dir)\n",
    "\n",
    "        for entity_name, entity_data in data['entities'].items():\n",
    "            if entity_name.startswith(\"DAF_\"):\n",
    "                continue\n",
    "\n",
    "            content = generate_entity_markdown(reg_name, entity_name, entity_data, entity_slug_set)\n",
    "            if content is None:\n",
    "                continue\n",
    "\n",
    "            safe_entity = canonical_entity_name(entity_name, reg_name)\n",
    "            out_path = reg_dir / f\"{safe_entity}.md\"\n",
    "            out_path.write_text(content, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Run it\n",
    "build_catalog('/Users/holmes/local_dev/semanticGIS/tests/semantictest/schemas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semanticGIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
